{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag\n",
    "import bs4\n",
    "import requests\n",
    "import json\n",
    "from pprint import pprint\n",
    "from time import sleep\n",
    "import tldextract\n",
    "from urllib.parse import urljoin\n",
    "visited = set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html_structure(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    classes = []\n",
    "    tags = {}\n",
    "    for tag in soup.find_all():\n",
    "        if isinstance(tag, Tag):\n",
    "            \n",
    "            #check if tag has class\n",
    "            if tag.has_attr('class'):\n",
    "                if tag.name not in tags:\n",
    "                    tags[tag.name] = []\n",
    "                    \n",
    "                # only add classes from tags that haven't been added yet to classes\n",
    "                for c in tag['class']:\n",
    "                    if c not in classes:\n",
    "                        classes.append(c)\n",
    "                        tags[tag.name].append(c)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_content(url, content, type='txt'):\n",
    "    filename = url.replace('/', '_').replace('https:', '').replace('http:', '') + \".\" + type\n",
    "    # filename = f\"{extracted.domain}_{extracted.suffix}.txt\"\n",
    "    path = \"data/faq/\" + type + \"/\" + filename\n",
    "    print(\"save_content path: \", path)\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_element(element):\n",
    "    # Handle string (NavigableString) elements\n",
    "    if isinstance(element, str) or isinstance(element, bs4.NavigableString):\n",
    "        return element\n",
    "\n",
    "    processed_elements = []\n",
    "    for child in element.children:\n",
    "        processed_elements.append(process_element(child))\n",
    "\n",
    "    # Now we handle various tag types\n",
    "    if element.name == 'a':\n",
    "        return f'[{\"\".join(processed_elements)}]({element.get(\"href\", \"\")})'\n",
    "    elif element.name == 'b':\n",
    "        return f'**{\"\".join(processed_elements)}**'\n",
    "    elif element.name == 'i':\n",
    "        return f'_{\"\".join(processed_elements)}_'\n",
    "    else:\n",
    "        return \"\".join(processed_elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://faq.leafwell.com/knowledge/'\n",
    "def valid_url(url):\n",
    "    # Implement this function to filter out URLs you don't want to visit\n",
    "    if \"facebook\" in url:\n",
    "        return False\n",
    "    if \"twitter\" in url:\n",
    "        return False\n",
    "    if \"instagram\" in url:\n",
    "        return False\n",
    "    if \"mailto\" in url:\n",
    "        return False\n",
    "\n",
    "    # if an url has \"blog\" in it, then it is valid\n",
    "    if \"knowledge\" in url:\n",
    "        if not url.startswith('https'):\n",
    "            url = urljoin(base_url, url)\n",
    "        return url\n",
    "    return False\n",
    "    # return \"http\" in url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    menu = soup.find(class_='kb-category-menu')\n",
    "    links = []\n",
    "    for link in menu.find_all('a'):\n",
    "        if link.has_attr('href'):\n",
    "            if valid_url(link['href']):\n",
    "                links.append(link['href'])\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited = set()\n",
    "def scrape_faq(url):\n",
    "    \n",
    "    url = valid_url(url)\n",
    "    if url is False:\n",
    "        return\n",
    "    if url in visited:\n",
    "        return\n",
    "    visited.add(url)\n",
    "    sleep(2)\n",
    "    print(\"scrape Url: \", url,\"\\n\\n\")\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    main_content = soup.find(class_='kb-content')\n",
    "\n",
    "    # Check if main content was found\n",
    "    if main_content is not None:\n",
    "        # Get 'kb-article tinymce-content' elements within the main content\n",
    "        kb_articles = main_content.find_all(class_='kb-article tinymce-content')\n",
    "        # Check if any 'kb-article tinymce-content' elements were found\n",
    "        if len(kb_articles) > 0:\n",
    "            for article in kb_articles:\n",
    "                # Get a list of dicts of all the links from the article\n",
    "                # Link dict = {\"text_of_link\": \"url_of_link\"}\n",
    "                Links = []\n",
    "                links = article.find_all('a')\n",
    "                # Loop through the links\n",
    "                for link in links:\n",
    "                    Links.append(json.dumps({link.get_text(): link.get('href')}))\n",
    "\n",
    "                # Get the text of the article\n",
    "                content = \" \".join(Links) + \"<||>\"\n",
    "                content += article.get_text()\n",
    "                # Save the content to a file\n",
    "                save_content(url, content, 'txt')\n",
    "                save_content(url, process_element(article), 'md')\n",
    "                # Print the content\n",
    "                print(content)\n",
    "                print('----------------------')\n",
    "        else:\n",
    "            # find links in the main content\n",
    "            links = main_content.find_all('a')\n",
    "            for link in links:\n",
    "                # Get the link\n",
    "                link_url = valid_url(link.get('href'))\n",
    "                # Check if the link is valid\n",
    "                if link_url:\n",
    "                    # Scrape the link\n",
    "                    scrape_faq(link_url)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://faq.leafwell.com/knowledge/general-information'\n",
    "links = get_category_links(url)\n",
    "# for link in links:\n",
    "#     scrape_faq(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_faq('https://faq.leafwell.com/knowledge/general-information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faq = 'https://faq.leafwell.com/knowledge/california-state-process'  # replace with your url\n",
    "# faq_structure = get_html_structure(faq)\n",
    "scrape_faq(faq)\n",
    "# print(json.dumps(faq_structure))\n",
    "# blog = 'https://leafwell.com/blog/is-marijuana-legal-in-bahrain/'\n",
    "# blog_structure = get_html_structure(blog)\n",
    "# print(json.dumps(blog_structure))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
